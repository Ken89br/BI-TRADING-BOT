# support_resistance.py
import pandas as pd
import numpy as np
from typing import Dict, Optional
from datetime import datetime, timedelta
import talib  # opcional (mantido por compatibilidade)
from scipy.stats import gaussian_kde
from sklearn.cluster import DBSCAN


class AdvancedSRAnalyzer:
    def __init__(self, data_client):
        """
        Analisador avançado de S/R com:
        - Machine Learning para clusterização
        - Análise de liquidez em tempo real
        - Integração multi-timeframe
        - Confirmação por volume e fluxo de ordens
        - Pivot Points tradicionais e ponderados por volume (VWAP pivots)
        """
        self.data_client = data_client
        self.cache = {}
        self.cache_expiry = {
            'scalping': timedelta(minutes=5),
            'daytrade': timedelta(minutes=15),
            'swing': timedelta(hours=1)
        }

    def _get_optimal_params(self, operation_type: str) -> dict:
        """Parâmetros otimizados por tipo de operação"""
        return {
            'scalping': {
                'base_tf': '1min',
                'aux_tf': '5min',
                'lookback': 120,  # 2 horas
                'tolerance_pct': 0.002,
                'volume_factor': 1.8,
                'min_cluster_size': 3,
                'min_touches': 2,
                'ema_window': 9,
                'liquidity_threshold': 1.5,
                'pivot_lookback': 30
            },
            'daytrade': {
                'base_tf': '5min',
                'aux_tf': '15min',
                'lookback': 96,  # 8 horas
                'tolerance_pct': 0.005,
                'volume_factor': 1.5,
                'min_cluster_size': 3,
                'min_touches': 2,
                'ema_window': 21,
                'liquidity_threshold': 1.3,
                'pivot_lookback': 24
            },
            'swing': {
                'base_tf': '1H',
                'aux_tf': '4H',
                'lookback': 168,  # 1 semana
                'tolerance_pct': 0.01,
                'volume_factor': 1.2,
                'min_cluster_size': 2,
                'min_touches': 2,
                'ema_window': 50,
                'liquidity_threshold': 1.1,
                'pivot_lookback': 24
            }
        }.get(operation_type)

    def _fetch_multi_timeframe_data(self, pair: str, params: dict) -> dict:
        """Obtém dados de múltiplos timeframes para análise conjunta"""
        base_data = self.data_client.fetch_candles(
            pair, interval=params['base_tf'], limit=params['lookback']
        )
        aux_data = self.data_client.fetch_candles(
            pair, interval=params['aux_tf'], limit=params['lookback'] // 4
        )
        pivot_data = self.data_client.fetch_candles(
            pair, interval=params['base_tf'], limit=params['pivot_lookback']
        )

        return {
            'base': self._process_raw_data(base_data),
            'aux': self._process_raw_data(aux_data),
            'pivot': self._process_raw_data(pivot_data)
        }

    def _process_raw_data(self, data: dict) -> pd.DataFrame:
        """Processa dados brutos em DataFrame padronizado"""
        if not data or "history" not in data:
            return pd.DataFrame()

        df = pd.DataFrame(data["history"])
        df = df.rename(columns={
            "open": "Open", "high": "High", "low": "Low",
            "close": "Close", "volume": "Volume"
        })

        for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # Timestamp -> Date (datetime)
        if 'timestamp' in df.columns:
            df['Date'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')
        else:
            # TODO: se o seu provedor usa outra coluna para tempo, ajuste aqui
            df['Date'] = pd.to_datetime(df.get('Date', pd.NaT), errors='coerce')

        df['Typical'] = (df['High'] + df['Low'] + df['Close']) / 3.0
        df['MA'] = df['Close'].ewm(span=9, adjust=False).mean()

        # TickVolume se existir; senão usa Volume como fallback
        if 'tickVolume' in df.columns:
            df['TickVolume'] = pd.to_numeric(df['tickVolume'], errors='coerce')
        elif 'TickVolume' not in df.columns:
            df['TickVolume'] = df['Volume']

        df = df.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume', 'Date'])
        return df

    def _calculate_pivot_points(self, df: pd.DataFrame) -> Dict[str, float]:
        """Calcula pivot points tradicionais (último candle completo)"""
        if len(df) < 2:
            return {}
        last_candle = df.iloc[-2] if len(df) > 1 else df.iloc[-1]
        high = float(last_candle['High'])
        low = float(last_candle['Low'])
        close = float(last_candle['Close'])

        pp = (high + low + close) / 3.0
        return {
            'pivot': pp,
            'r1': 2 * pp - low,
            's1': 2 * pp - high,
            'r2': pp + (high - low),
            's2': pp - (high - low),
            'r3': high + 2 * (pp - low),
            's3': low - 2 * (high - pp),
            'pivot_high': high,
            'pivot_low': low
        }

    def _calculate_adaptive_vwap(
        self,
        df: pd.DataFrame,
        operation_type: str,
        reset_mode: str = "session",
        use_tick_volume: bool = True
    ) -> dict:
        """
        VWAP adaptativo:
        - Reset configurável (session/daily/continuous)
        - Bandas dinâmicas por timeframe
        - Detecção de tendência
        """
        # df vazio
        if df.empty:
            return {
                'vwap': 0.0,
                'trend': 'sideways',
                'bands': {'upper1': 0.0, 'lower1': 0.0, 'upper2': 0.0, 'lower2': 0.0},
                'slope': 0.0
            }

        # Configurações por tipo de operação
        tf_settings = {
            'scalping': {'std_period': 15, 'n_std_1': 1.0, 'n_std_2': 1.5},
            'daytrade': {'std_period': 20, 'n_std_1': 1.5, 'n_std_2': 2.0},
            'swing': {'std_period': 50, 'n_std_1': 2.0, 'n_std_2': 2.5}
        }.get(operation_type, {'std_period': 20, 'n_std_1': 1.5, 'n_std_2': 2.0})

        # Typical e Volume a usar
        typical = (df['High'] + df['Low'] + df['Close']) / 3.0
        if use_tick_volume and 'TickVolume' in df.columns:
            volume = df['TickVolume']
        else:
            volume = df['Volume']

        # Garantir Date em datetime para resets que dependem de tempo
        if not pd.api.types.is_datetime64_any_dtype(df['Date']):
            df = df.copy()
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

        # Cálculo do VWAP com reset
        if reset_mode == "continuous":
            cum_vol = volume.cumsum()
            cum_vol_price = (typical * volume).cumsum()
            vwap_series = cum_vol_price / cum_vol

        elif reset_mode == "daily":
            dates = df['Date'].dt.date
            vwap_series = (typical * volume).groupby(dates).cumsum() / volume.groupby(dates).cumsum()

        elif reset_mode == "session":
            if 'Session' not in df.columns:
                df = df.copy()
                # garante que Date é datetime antes do diff()
                if not pd.api.types.is_datetime64_any_dtype(df['Date']):
                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
                df['Session'] = (df['Date'].diff() > pd.Timedelta('1h')).cumsum()
            vwap_series = (typical * volume).groupby(df['Session']).cumsum() / volume.groupby(df['Session']).cumsum()

        else:
            raise ValueError("reset_mode deve ser 'continuous', 'daily' ou 'session'")

        # Detecção de tendência
        close_price = float(df['Close'].iloc[-1])
        vwap_now = float(vwap_series.iloc[-1]) if len(vwap_series) else 0.0
        vwap_slope = float(vwap_series.iloc[-1] - vwap_series.iloc[-5]) if len(vwap_series) > 5 else 0.0

        if close_price > vwap_now and vwap_slope > 0:
            trend = "up"
        elif close_price < vwap_now and vwap_slope < 0:
            trend = "down"
        else:
            trend = "sideways"

        # Bandas de volatilidade (com fallback seguro)
        std_period = int(tf_settings['std_period'])
        if len(typical) >= std_period:
            std_val = float(typical.rolling(std_period).std().iloc[-1])
        else:
            std_val = float(typical.std()) if len(typical) else 0.0

        bands = {
            'upper1': vwap_now + tf_settings['n_std_1'] * std_val,
            'lower1': vwap_now - tf_settings['n_std_1'] * std_val,
            'upper2': vwap_now + tf_settings['n_std_2'] * std_val,
            'lower2': vwap_now - tf_settings['n_std_2'] * std_val
        }

        return {
            'vwap': vwap_now,
            'trend': trend,
            'bands': {k: float(v) for k, v in bands.items()},
            'slope': vwap_slope
        }

    def _calculate_volume_nodes(self, df: pd.DataFrame) -> dict:
        """Calcula nós de volume via KDE (POC, Value Area)"""
        if df.empty or len(df) < 10:
            return {}

        typical_prices = df['Typical'].values
        volumes = df['Volume'].values

        kde = gaussian_kde(typical_prices, weights=volumes)
        x = np.linspace(float(np.min(typical_prices)), float(np.max(typical_prices)), 100)
        y = kde(x)

        # picos (POCs)
        peaks = np.where((y[1:-1] > y[:-2]) & (y[1:-1] > y[2:]))[0] + 1
        if len(peaks) == 0:
            return {}

        main_poc = float(x[peaks[np.argmax(y[peaks])]])

        # Value Area ~70% (heurística)
        total_density = float(np.sum(y))
        sorted_areas = sorted(zip(x, y), key=lambda v: -v[1])
        cum_density = 0.0
        value_area = []
        for price, density in sorted_areas:
            if cum_density >= 0.7 * total_density:
                break
            value_area.append(float(price))
            cum_density += float(density)

        return {
            'poc': main_poc,
            'value_area': (min(value_area), max(value_area)) if value_area else None,
            'volume_distribution': list(zip(x.tolist(), y.tolist()))
        }

    def _find_liquidity_zones(self, df: pd.DataFrame, params: dict) -> dict:
        """
        Detecta zonas de liquidez (order blocks) com mitigação, força adaptativa
        e opção de range ('body' ou 'full').
        Retorna: {'zones': [ {type, range, time, volume, strength} ]}
        """
        if df.empty or len(df) < 40:
            return {'zones': []}

        liquidity_zones = []
        vol_ma = df['Volume'].rolling(20).mean()

        for i in range(3, len(df) - 3):
            current_vol = df['Volume'].iloc[i]
            if current_vol <= vol_ma.iloc[i] * params.get('liquidity_threshold', 1.5):
                continue

            # Bullish OB
            if (df['Close'].iloc[i] < df['Open'].iloc[i] and
                df['Close'].iloc[i + 1] > df['High'].iloc[i] and
                df['Close'].iloc[i + 2] > df['High'].iloc[i]):

                if params.get("range_mode", "body") == "body":
                    zone_range = (float(df['Low'].iloc[i]), float(df['Open'].iloc[i]))
                else:
                    zone_range = (float(df['Low'].iloc[i]), float(df['High'].iloc[i]))

                breakout_strength = (df['Close'].iloc[i + 1] - df['High'].iloc[i]) / df['High'].iloc[i]
                strength = (
                    0.5 * (current_vol / vol_ma.iloc[i]) +
                    0.3 * breakout_strength
                )

                ref_levels = [params.get("use_vwap"), params.get("use_pivot")]
                ref_levels = [lvl for lvl in ref_levels if lvl is not None]
                if ref_levels:
                    dist = min([abs(df['Close'].iloc[i] - lvl) / lvl for lvl in ref_levels])
                    strength += max(0, 0.2 * (1 - dist))

                liquidity_zones.append({
                    'type': 'bullish',
                    'range': zone_range,
                    'time': df['Date'].iloc[i],
                    'volume': float(current_vol),
                    'strength': round(float(strength), 3)
                })

            # Bearish OB
            if (df['Close'].iloc[i] > df['Open'].iloc[i] and
                df['Close'].iloc[i + 1] < df['Low'].iloc[i] and
                df['Close'].iloc[i + 2] < df['Low'].iloc[i]):

                if params.get("range_mode", "body") == "body":
                    zone_range = (float(df['Open'].iloc[i]), float(df['High'].iloc[i]))
                else:
                    zone_range = (float(df['Low'].iloc[i]), float(df['High'].iloc[i]))

                breakout_strength = (df['Low'].iloc[i] - df['Close'].iloc[i + 1]) / df['Low'].iloc[i]
                strength = (
                    0.5 * (current_vol / vol_ma.iloc[i]) +
                    0.3 * breakout_strength
                )

                ref_levels = [params.get("use_vwap"), params.get("use_pivot")]
                ref_levels = [lvl for lvl in ref_levels if lvl is not None]
                if ref_levels:
                    dist = min([abs(df['Close'].iloc[i] - lvl) / lvl for lvl in ref_levels])
                    strength += max(0, 0.2 * (1 - dist))

                liquidity_zones.append({
                    'type': 'bearish',
                    'range': zone_range,
                    'time': df['Date'].iloc[i],
                    'volume': float(current_vol),
                    'strength': round(float(strength), 3)
                })

        # Mitigação: remove zonas já tocadas
        non_mitigated = []
        for z in liquidity_zones:
            after_zone = df[df['Date'] > z['time']]
            touched = ((after_zone['Low'] <= z['range'][1]) &
                       (after_zone['High'] >= z['range'][0])).any()
            if not touched:
                non_mitigated.append(z)

        min_strength = params.get("min_strength", 0.8)
        strong_zones = [z for z in non_mitigated if z['strength'] >= min_strength]
        return {'zones': strong_zones[-5:]}

    def _find_valid_fractals(self, df: pd.DataFrame, params: dict) -> dict:
        """Identifica fractais válidos com confirmação de volume"""
        fractals = {'supports': [], 'resistances': []}
        if df.empty or len(df) < 7:
            return fractals

        for i in range(2, len(df) - 2):
            vol_ok = df['Volume'].iloc[i] >= df['Volume'].rolling(5).mean().iloc[i] * params['volume_factor']

            # Suporte
            if (df['Low'].iloc[i] < df['Low'].iloc[i - 1] and
                df['Low'].iloc[i] < df['Low'].iloc[i - 2] and
                df['Low'].iloc[i] < df['Low'].iloc[i + 1] and
                df['Low'].iloc[i] < df['Low'].iloc[i + 2] and
                vol_ok):
                fractals['supports'].append(float(df['Low'].iloc[i]))

            # Resistência
            if (df['High'].iloc[i] > df['High'].iloc[i - 1] and
                df['High'].iloc[i] > df['High'].iloc[i - 2] and
                df['High'].iloc[i] > df['High'].iloc[i + 1] and
                df['High'].iloc[i] > df['High'].iloc[i + 2] and
                vol_ok):
                fractals['resistances'].append(float(df['High'].iloc[i]))

        return fractals

    def _cluster_price_levels(self, prices: list, current_price: float, params: dict) -> list:
        """Clusteriza preços usando DBSCAN com epsilon adaptativo"""
        if not prices or len(prices) < params['min_cluster_size']:
            return []
        X = np.array(prices).reshape(-1, 1)

        adaptive_eps = []
        for p in prices:
            distance_pct = abs(p - current_price) / current_price
            eps = params['tolerance_pct'] * current_price * (1 + distance_pct * 3)
            adaptive_eps.append(eps)

        epsilon = float(np.median(adaptive_eps))
        db = DBSCAN(eps=epsilon, min_samples=params['min_cluster_size']).fit(X)

        clusters = {}
        for i, label in enumerate(db.labels_):
            if label == -1:
                continue
            clusters.setdefault(label, []).append(prices[i])

        return [round(sum(cluster) / len(cluster), 5) for cluster in clusters.values()]

    def _get_adaptive_ema_span(self, timeframe: str, operation_type: str) -> int:
        """Retorna EMA span ideal por timeframe e operação"""
        timeframe_settings = {
            '30s': {'scalping': 5, 'daytrade': 8, 'swing': 12},
            '1min': {'scalping': 8, 'daytrade': 12, 'swing': 20},
            '2min': {'scalping': 10, 'daytrade': 15, 'swing': 25},
            '5min': {'scalping': 12, 'daytrade': 20, 'swing': 30},
            '10min': {'scalping': 15, 'daytrade': 25, 'swing': 40},
            '15min': {'scalping': 20, 'daytrade': 30, 'swing': 50},
            '30min': {'scalping': 25, 'daytrade': 40, 'swing': 60},
            '1H': {'scalping': 30, 'daytrade': 50, 'swing': 80},
            '4H': {'scalping': 40, 'daytrade': 60, 'swing': 100}
        }
        default_settings = {'scalping': 15, 'daytrade': 25, 'swing': 40}
        return timeframe_settings.get(timeframe, default_settings).get(operation_type, 20)

    def _calculate_fibonacci_extensions(
        self,
        df: pd.DataFrame,
        supports: list,
        resistances: list,
        timeframe: str = "15min",
        operation_type: str = "daytrade"
    ) -> dict:
        """
        Calcula níveis de Fibonacci (retracements + extensions):
        - Tendência via EMA adaptativa
        - Swings de fractais clusterizados (com filtro de tendência)
        - Fallback robusto
        """
        if df.empty:
            return {}

        ema_span = self._get_adaptive_ema_span(timeframe, operation_type)
        current_price = float(df['Close'].iloc[-1])
        ema_val = float(df['Close'].ewm(span=ema_span).mean().iloc[-1])
        trend = "up" if current_price > ema_val else "down"

        swing_high, swing_low = None, None

        # Swings pelos fractais (com filtro por tendência)
        try:
            if len(supports) >= 2 and len(resistances) >= 2:
                if trend == "up":
                    swing_low = min(supports[-3:])
                    valid_highs = [r for r in resistances if r > swing_low and r > current_price]
                    swing_high = max(valid_highs[-3:]) if valid_highs else None
                else:
                    swing_high = max(resistances[-3:])
                    valid_lows = [s for s in supports if s < swing_high and s < current_price]
                    swing_low = min(valid_lows[-3:]) if valid_lows else None
        except Exception as e:
            print(f"⚠️ Erro no cálculo de swings: {e}")

        # Fallback adaptativo
        if swing_high is None or swing_low is None:
            period = 10 if len(df) >= 10 else len(df)
            swing_high = float(df['High'].rolling(period).max().iloc[-1])
            swing_low = float(df['Low'].rolling(period).min().iloc[-1])

        if swing_high <= swing_low:
            swing_high, swing_low = max(swing_high, swing_low), min(swing_high, swing_low)

        diff = float(swing_high - swing_low)
        if diff <= 0:
            return {}

        if trend == "up":
            retracements = {
                '236': swing_high - diff * 0.236,
                '382': swing_high - diff * 0.382,
                '500': swing_high - diff * 0.5,
                '618': swing_high - diff * 0.618,
                '786': swing_high - diff * 0.786
            }
            extensions = {
                '1000': swing_high,
                '1272': swing_high + diff * 0.272,
                '1414': swing_high + diff * 0.414,
                '1618': swing_high + diff * 0.618,
                '2000': swing_high + diff * 1.0,
                '2618': swing_high + diff * 1.618,
                '3618': swing_high + diff * 2.618
            }
        else:
            retracements = {
                '236': swing_low + diff * 0.236,
     
